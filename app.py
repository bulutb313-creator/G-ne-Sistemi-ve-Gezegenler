import streamlit as st
import os
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.vectorstores import Chroma
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.retrievers import MultiQueryRetriever
import pdfplumber 

# --- RAG ZÄ°NCÄ°RÄ°NÄ° BAÅLATAN FONKSÄ°YON ---
@st.cache_resource
def get_rag_chain():
    PDF_DOSYA_ADI = "sistem.pdf"
    file_path = PDF_DOSYA_ADI

    if not os.path.exists(file_path):
        st.error(f"KRÄ°TÄ°K HATA: '{file_path}' dosyasÄ± bulunamadÄ±. LÃ¼tfen dosyanÄ±n GitHub deponuzda yÃ¼klÃ¼ olduÄŸundan emin olun.")
        return None
    try:
        # 1. Veri Ä°ÅŸleme
        full_text = "";
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages: full_text += page.extract_text() + "\n\n"
        documents = [Document(page_content=full_text, metadata={"source": file_path})]
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, overlap=200, separators=["\n\n", "\n", " ", ""],)
        texts = text_splitter.split_documents(documents)

        # 2. VektÃ¶r VeritabanÄ± OluÅŸturma
        embedding_model = GoogleGenerativeAIEmbeddings(model="text-embedding-004")
        vectorstore = Chroma.from_documents(documents=texts, embedding=embedding_model)

        # 3. RAG Zinciri Kurulumu
        llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.2)
        base_retriever = vectorstore.as_retriever(search_kwargs={"k": 8})
        retriever = MultiQueryRetriever.from_llm(retriever=base_retriever, llm=llm)

        # PROMPT ÅABLONU (Analitik ve Zeki)
        prompt_template = """Sen bir GÃœNEÅ SÄ°STEMÄ° VE JEOFIZIK UZMANISIN. GÃ¶revin, sana verilen BAÄLAM'daki bilgileri ANALÄ°Z EDEREK bir cevap SENTEZLEMEKTÄ°R. Neden-sonuÃ§ iliÅŸkileri kur, kÄ±yaslamalar yap ve mantÄ±ksal Ã§Ä±karÄ±mlar sun.

        EÄŸer cevap KESÄ°NLÄ°KLE BAÄLAM'da yoksa, sadece "Bu konuda elimde yeterli bilgi yok." diye cevap ver.
        BAÄLAM: {context}
        Soru: {question}
        Cevap:"""
        PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

        qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever, chain_type_kwargs={"prompt": PROMPT})
        return qa_chain

    except Exception as e:
        st.error(f"RAG Zinciri KurulamadÄ±: LÃ¼tfen GitHub deponuzdaki dosyalarÄ± kontrol edin. Hata: {e}")
        return None

# --- STREAMLIT ARAYÃœZÃœ ANA FONKSÄ°YONU ---
def main():
    st.set_page_config(page_title="Analitik Chatbot", layout="wide")
    st.title("ğŸš€ GÃœNEÅ SÄ°STEMÄ° VE GEZEGENLER RAG Chatbot (Analitik Uzman) ğŸª")
    st.caption("Cevaplar 'sistem.pdf' dosyasÄ±ndaki TÃœM bilgilerden analitik sentezlenmiÅŸtir. ğŸ›°ï¸")

    # API AnahtarÄ±nÄ± al ve kontrol et (Ekrandaki hatayÄ± Ã§Ã¶zen kod)
    if "GEMINI_API_KEY" not in os.environ:
        st.error("LÃ¼tfen Gemini API AnahtarÄ±nÄ±zÄ± giriniz. Anahtar olmadan sistem Ã§alÄ±ÅŸmaz.")
        key = st.text_input("Gemini API AnahtarÄ±:", type="password", key="api_input") # Etiket: Gemini
        if key:
            os.environ["GEMINI_API_KEY"] = key
            st.rerun()
        return

    # Chain'i baÅŸlat
    if "qa_chain" not in st.session_state:
        st.session_state.qa_chain = get_rag_chain()
        if st.session_state.qa_chain is None: return

    if "messages" not in st.session_state: st.session_state.messages = []
    for message in st.session_state.messages:
        with st.chat_message(message["role"]): st.markdown(message["content"])

    if prompt := st.chat_input("GÃ¼neÅŸ Sistemi hakkÄ±nda analitik bir soru sorun..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"): st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("Analiz ediliyor... Neden-sonuÃ§ iliÅŸkileri kuruluyor... ğŸ§ "):
                try:
                    answer = st.session_state.qa_chain.run(prompt)
                    st.markdown(answer)
                    st.session_state.messages.append({"role": "assistant", "content": answer})
                except Exception as e:
                    st.error(f"API hatasÄ± oluÅŸtu veya zincir Ã§alÄ±ÅŸtÄ±rÄ±lamadÄ±. Hata: {e}")

if __name__ == "__main__":
    main()
