import streamlit as st
import os
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.vectorstores import Chroma
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.retrievers import MultiQueryRetriever
import pdfplumber 

@st.cache_resource
def get_rag_chain():
    # PDF DosyasÄ± KontrolÃ¼ ve Yolu
    PDF_DOSYA_ADI = "sistem.pdf"
    file_path = PDF_DOSYA_ADI

    if not os.path.exists(file_path):
        # Streamlit Cloud'da dosya bulunamazsa kritik hata verir
        st.error(f"KRÄ°TÄ°K HATA: '{file_path}' dosyasÄ± bulunamadÄ±. LÃ¼tfen dosyanÄ±n GitHub deponuzda ve Streamlit Cloud'a yÃ¼klÃ¼ olduÄŸundan emin olun.")
        return None

    try:
        # 1. Veri Ä°ÅŸleme (PDF Okuma ve Chunking)
        full_text = ""; full_text = "";
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages: full_text += page.extract_text() + "\n\n"
        documents = [Document(page_content=full_text, metadata={"source": file_path})]
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, separators=["\n\n", "\n", " ", ""],)
        texts = text_splitter.split_documents(documents)

        # 2. VektÃ¶r VeritabanÄ± OluÅŸturma
        embedding_model = GoogleGenerativeAIEmbeddings(model="text-embedding-004")
        vectorstore = Chroma.from_documents(documents=texts, embedding=embedding_model)

        # 3. RAG Zinciri Kurulumu (MultiQuery ile)
        llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.2)
        base_retriever = vectorstore.as_retriever(search_kwargs={"k": 8})
        retriever = MultiQueryRetriever.from_llm(retriever=base_retriever, llm=llm)

        # PROMPT ÅABLONU
        prompt_template = """Sen bir GÃœNEÅ SÄ°STEMÄ° VE JEOFIZIK UZMANISIN. GÃ¶revin, sana verilen BAÄLAM'daki TÃœM bilgileri kullanarak bir cevap SENTEZLEMEKTÄ°R. Asla hemen "bilmiyorum" deme. Cevap verebilmek iÃ§in, Ã§ekilen baÄŸlamdaki bilgileri dikkatlice oku, neden-sonuÃ§ iliÅŸkilerini kur ve TÃœM Ä°LGÄ°LÄ° DETAYLARI maddeler halinde veya akÄ±cÄ± paragraflarla aktar.

EÄŸer BAÄLAM, soruyu cevaplamak iÃ§in KESÄ°NLÄ°KLE YETERSÄ°Z veya EKSÄ°K ise, bu durumda sadece "Bu konuda elimde yeterli bilgi yok." diye cevap ver.

BAÄLAM: {context}

Soru: {question}
Cevap:"""
        PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

        qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever, chain_type_kwargs={"prompt": PROMPT})
        return qa_chain

    except Exception as e:
        st.error(f"RAG Zinciri KurulamadÄ±: {e}")
        return None

# --- STREAMLIT ARAYÃœZÃœ ANA FONKSÄ°YONU ---
def main():
    st.title("ğŸš€ GÃœNEÅ SÄ°STEMÄ° VE GEZEGENLER RAG Chatbot ğŸª")
    st.caption("Cevaplar 'sistem.pdf' dosyasÄ±ndaki TÃœM bilgilerden alÄ±nmÄ±ÅŸtÄ±r. ğŸ›°ï¸")

    if "GEMINI_API_KEY" not in os.environ:
        st.error("LÃ¼tfen Gemini API AnahtarÄ±nÄ±zÄ± giriniz. Anahtar olmadan sistem Ã§alÄ±ÅŸmaz.")
        key = st.text_input("API AnahtarÄ±:", type="password")
        if key:
            os.environ["GEMINI_API_KEY"] = key
            st.rerun()
        return

    if "qa_chain" not in st.session_state:
        st.session_state.qa_chain = get_rag_chain()
        if st.session_state.qa_chain is None: return

    if "messages" not in st.session_state: st.session_state.messages = []
    for message in st.session_state.messages:
        with st.chat_message(message["role"]): st.markdown(message["content"])

    if prompt := st.chat_input("GÃ¼neÅŸ Sistemi hakkÄ±nda bir soru sorun..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"): st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("Cevap aranÄ±yor... ğŸŒ‘... ğŸª... â˜„ï¸"):
                try:
                    answer = st.session_state.qa_chain.run(prompt)
                    st.markdown(answer)
                    st.session_state.messages.append({"role": "assistant", "content": answer})
                except Exception as e:
                    st.error(f"API hatasÄ± oluÅŸtu veya zincir kurulamadÄ±: {e}")

if __name__ == "__main__":
    main()
