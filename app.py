import streamlit as st
import os
import pdfplumber 

# --- Hata Giderilmi≈ü ve G√ºncel K√ºt√ºphane Yollarƒ± ---
from langchain_core.prompts import PromptTemplate 
from langchain_core.documents import Document 
from langchain_text_splitters import RecursiveCharacterTextSplitter 
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import Chroma 
from langchain.chains import RetrievalQA 
from langchain.retrievers.multi_query import MultiQueryRetriever # <-- Sƒ∞Zƒ∞N KODUNUZDAKƒ∞ SON HATA BU SATIRDI. ARTIK √á√ñZ√úLD√ú.


# --- RAG Zƒ∞NCƒ∞Rƒ∞Nƒ∞ BA≈ûLATAN FONKSƒ∞YON ---
@st.cache_resource
def get_rag_chain():
    """RAG sistemini kurar ve zinciri d√∂nd√ºr√ºr."""
    PDF_DOSYA_ADI = "sistem.pdf"
    file_path = PDF_DOSYA_ADI

    if not os.path.exists(file_path):
        st.error(f"KRƒ∞Tƒ∞K HATA: '{file_path}' dosyasƒ± GitHub'da bulunamƒ±yor.")
        return None
    try:
        # 1. VERƒ∞ ƒ∞≈ûLEME
        full_text = "";
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages: full_text += page.extract_text() + "\n\n"
        documents = [Document(page_content=full_text, metadata={"source": file_path})]
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, overlap=200, separators=["\n\n", "\n", " ", ""],)
        texts = text_splitter.split_documents(documents)

        # 2. Vekt√∂r Veritabanƒ± Olu≈üturma
        embedding_model = GoogleGenerativeAIEmbeddings(model="text-embedding-004")
        vectorstore = Chroma.from_documents(documents=texts, embedding=embedding_model)

        # 3. RAG Zinciri Kurulumu
        llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.2)
        base_retriever = vectorstore.as_retriever(search_kwargs={"k": 8})
        # MultiQueryRetriever kullanƒ±mƒ±
        retriever = MultiQueryRetriever.from_llm(retriever=base_retriever, llm=llm)

        # ANALƒ∞Tƒ∞K PROMPT ≈ûABLONU
        prompt_template = """Sen bir G√úNE≈û Sƒ∞STEMƒ∞ VE JEOFIZIK UZMANISIN. G√∂revin, sana verilen BAƒûLAM'daki bilgileri ANALƒ∞Z EDEREK bir cevap SENTEZLEMEKTƒ∞R. Neden-sonu√ß ili≈ükileri kur, kƒ±yaslamalar yap ve mantƒ±ksal √ßƒ±karƒ±mlar sun.

        Eƒüer cevap KESƒ∞NLƒ∞KLE BAƒûLAM'da yoksa, sadece "Bu konuda elimde yeterli bilgi yok." diye cevap ver.
        BAƒûLAM: {context}
        Soru: {question}
        Cevap:"""
        PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

        qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever, chain_type_kwargs={"prompt": PROMPT})
        return qa_chain

    except Exception as e:
        st.error(f"RAG Zinciri Kurulamadƒ±. L√ºtfen API Anahtarƒ±nƒ±zƒ± ve GitHub dosyalarƒ±nƒ±zƒ± kontrol edin. Hata: {e}")
        return None

# --- STREAMLIT ARAY√úZ√ú ANA FONKSƒ∞YONU ---
def main():
    st.set_page_config(page_title="Analitik Chatbot", layout="wide")
    st.title("üöÄ G√úNE≈û Sƒ∞STEMƒ∞ VE GEZEGENLER RAG Chatbot (Analitik Uzman) ü™ê")
    st.caption("Cevaplar 'sistem.pdf' dosyasƒ±ndaki T√úM bilgilerden analitik sentezlenmi≈ütir. Streamlit Cloud'da kalƒ±cƒ± yayƒ±nda. üõ∞Ô∏è")

    # API Anahtarƒ±nƒ± al ve kontrol et (Secrets'ƒ± okur)
    if "GEMINI_API_KEY" not in os.environ:
        st.error("L√ºtfen Gemini API Anahtarƒ±nƒ±zƒ± giriniz. Anahtar olmadan sistem √ßalƒ±≈ümaz.")
        key = st.text_input("Gemini API Anahtarƒ±:", type="password", key="api_input") 
        if key:
            os.environ["GEMINI_API_KEY"] = key
            st.rerun()
        return

    # Chain'i ba≈ülat
    if "qa_chain" not in st.session_state:
        st.session_state.qa_chain = get_rag_chain()
        if st.session_state.qa_chain is None: return

    if "messages" not in st.session_state: st.session_state.messages = []
    for message in st.session_state.messages:
        with st.chat_message(message["role"]): st.markdown(message["content"])

    if prompt := st.chat_input("G√ºne≈ü Sistemi hakkƒ±nda analitik bir soru sorun..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"): st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("Analiz ediliyor... Neden-sonu√ß ili≈ükileri kuruluyor... üß†"):
                try:
                    answer = st.session_state.qa_chain.run(prompt)
                    st.markdown(answer)
                    st.session_state.messages.append({"role": "assistant", "content": answer})
                except Exception as e:
                    st.error(f"API hatasƒ± olu≈ütu veya zincir √ßalƒ±≈ütƒ±rƒ±lamadƒ±. Hata: {e}")

if __name__ == "__main__":
    main()
